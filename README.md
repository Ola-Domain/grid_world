Hereâ€™s the **updated README** that includes the new stress testing, hyperparameter sweep, and evaluation graphs you ran â€” this integrates everything youâ€™ve worked hard on:

---

# Deep Q-Learning GridWorld Agent

This project is a **visual, interactive, and analytics-rich implementation** of a **Q-Learning agent** solving a 5Ã—5 **GridWorld** environment using **Reinforcement Learning (RL)**. Built with **Python**, **Pygame**, and **Seaborn/Matplotlib**, it features live visualizations, sound effects, and deep evaluation metrics to track the agentâ€™s learning process.

---

## ğŸ§  What is Reinforcement Learning?

Reinforcement Learning (RL) is a branch of Machine Learning where an agent learns to make decisions by interacting with an environment to maximise cumulative rewards. In this project:

* The agent learns **which actions lead to the highest long-term gains**.
* A **Q-table** stores estimated rewards (Q-values) for each state-action pair.
* An **epsilon-greedy strategy** balances **exploration** (trying new things) and **exploitation** (using known best actions).

---

## âœ¨ Features

* âœ… 5Ã—5 bounded GridWorld
* âœ… Special Jump (2,4 â†’ 4,4) with +5 reward
* âœ… Terminal goal (5,5) with +10 reward
* âœ… Obstacles blocking movement
* âœ… Tabular Q-learning with dynamic epsilon decay
* âœ… Sound effects on jump and goal events
* âœ… Sprite-based grid with live HUD showing stats
* âœ… Automatic early stopping when avg reward >10 over 30 episodes
* âœ… Stress tests: no jump, random obstacles
* âœ… Hyperparameter sweep on Î±, Î³, Îµ-decay
* âœ… Plots and heatmaps for learning curves, epsilon decay, convergence

---

## ğŸ“ File Structure

```
DeepQ-GridWorld/
â”œâ”€â”€ assets/
â”œâ”€â”€ config.py
â”œâ”€â”€ environment.py
â”œâ”€â”€ q_learning.py
â”œâ”€â”€ game.py
â”œâ”€â”€ main.py
â”œâ”€â”€ stress_test_sweep.py
â”œâ”€â”€ plot_sweep_results.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ q_table.json
â”‚   â”œâ”€â”€ policy_map.png
â”‚   â”œâ”€â”€ value_heatmap.png
â”‚   â”œâ”€â”€ sweep_results.json
â”‚   â”œâ”€â”€ [training graphs].png
â””â”€â”€ README.md
```

---

## ğŸš€ Setup & Run

```bash
git clone https://github.com/Ola-Domain/grid_world.git
cd grid_world
pip install pygame pandas matplotlib seaborn
python main.py
```

---

## ğŸ¯ Environment Rules

* **Grid Size:** 5Ã—5
* **Start:** (2,1)
* **Terminal:** (5,5), reward +10
* **Jump Tile:** (2,4 â†’ 4,4), reward +5
* **Obstacles:** (3,3), (4,3), (3,4), (3,5)
* **Other moves:** reward -1
* **Actions:** North=1, South=2, East=3, West=4

---

## ğŸ¤– Q-Learning Summary

* Update rule:
  *Q(s,a) â† Q(s,a) + Î± \[r + Î³ max(Q(s',a')) âˆ’ Q(s,a)]*
* Parameters:

  * Learning rate (Î±): how fast Q-values update
  * Discount factor (Î³): weight on future rewards
  * Epsilon (Îµ): exploration rate, decays over time
* Q-table initialized to zero, updated each step

---

## ğŸ“Š Evaluation and Results

* **Live Metrics in HUD:**

  * Episode number
  * Total reward
  * Epsilon value
  * Average reward over last 30 episodes
  * Average episode time

* **Generated Outputs:**

  * ğŸ‰ `results/q_table.json`
  * ğŸ“½ï¸ `results/final_episode_replay.mp4`
  * ğŸ§­ `results/policy_map.png`
  * ğŸŒ¡ï¸ `results/value_heatmap.png`

* **Stress Testing & Hyperparameter Sweep:**

  * Baseline, no-jump, random-obstacle variants
  * Learning curves, epsilon decay, convergence plots
  * Alpha/gamma sweep with heatmaps

---

## ğŸ“ˆ Evaluation Plots

* Learning curve: total reward per episode
* Epsilon decay curve: exploration â†’ exploitation
* Winning percentage over episodes
* Total reward vs. steps
* Average steps per episode
* Q-value distribution
* Final policy map (best action per state)
* Hyperparameter sweep: heatmaps and bar plots

---

## ğŸ”§ Customization Tips

* Replace textures and sounds in `assets/`
* Modify hyperparameters in `q_learning.py`
* Change reward settings in `environment.py`
* Extend with Deep Q-Network (DQN) in future

---

## ğŸ’¥ Future Work

* Save/load Q-table between runs
* Add stochastic rewards or transitions
* Implement ablation tests (disable jump, random obstacles)
* Integrate DQN with neural networks
* Record gameplay videos/GIFs
* Add manual mode for human-agent comparison

---

## ğŸ“Œ References

* Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.)
* Watkins, C. J. C. H., & Dayan, P. (1992). *Q-learning*. Machine Learning, 8(3â€“4), 279â€“292
* Ulster University COM762 Lecture Slides (2024)
* GitHub: [https://github.com/Ola-Domain/grid\_world](https://github.com/Ola-Domain/grid_world)

---

## ğŸ“¬ Contact

* **Sulaimon Oladotun Olasupo**
* 
  Computer Science and Technology
  Ulster University, Birmingham, UK
* ğŸ“§ [olasupooladotun6@gmail.com](mailto:olasupooladotun6@gmail.com)
* 
  ğŸ’» [GitHub Repository](https://github.com/Ola-Domain/grid_world)

---

Happy training, testing, and breaking your agents! ğŸ§ ğŸ¯ğŸ”¥

---

